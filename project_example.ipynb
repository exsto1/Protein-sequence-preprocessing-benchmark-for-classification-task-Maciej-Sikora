{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Benchmark comparison of protein sequence preprocessing effect on learning task for Pfam family classification\n",
    "\n",
    "Notebook below allows to play with parameters of the workflow and yield different results.\n",
    "As a default state it is set with parameters described in the paper as well as used as a base for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, interact_manual, fixed\n",
    "import sys\n",
    "from urllib import request\n",
    "from scripts.data_preparation_support import data_preparation, split_to_train_test\n",
    "from scripts.shorten_encoding import compress_protein_data_original, compress_protein_data_singletons, compress_protein_data_triplets, compress_protein_data_sum_of_triplets, compress_protein_data_sum_of_k_mers\n",
    "from scripts.prepare_vectors import split_data_to_classes, prepare_biovec_model, load_biovec_model_with_classes\n",
    "from scripts.prepare_input_stats import input_analysis\n",
    "from scripts.model_scripts.decision_trees import decision_tree_func\n",
    "from scripts.model_scripts.random_tree import random_tree_func\n",
    "from scripts.model_scripts.mlp import mlp_func\n",
    "from scripts.model_scripts.nearest_neighbours import nearest_neighbours_func\n",
    "from scripts.model_scripts.deep_learning import deep_learning_func\n",
    "from scripts.plotting_support import results_plot_benchmark, plot_sizes\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "In this project I provide already prepared - cleaned dataset used for the report, so download part below is commented out. However, if desired, parameters can be tweaked to obtain altered version.\n",
    "\n",
    "Raw data for this project is easily accessible by the Swissprot part of the Uniprot Database.\n",
    "Unfortunately, full file weights around 250Mb, so instead I provide link for download.\n",
    "\n",
    "In case server can't handle the download through Python, it is still possiblle to download tab-separated file directly\n",
    "from the website: [LINK](https://www.uniprot.org/uniprot/?query=reviewed%3Ayes&columns=id%2Cdatabase(Pfam)%2Corganism%2Csequence)\n",
    "\n",
    "It suggested, although not neccessary, to put downloaded file into ./data/full directory for clarity and ease of use of the default values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# url=\"https://www.uniprot.org/uniprot/?query=reviewed:yes&format=tab&columns=id,database(Pfam),organism,sequence\"\n",
    "# request.urlretrieve(url, \"./data/full/uniprot-reviewed_yes.tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_prepare_widget():\n",
    "    org_w = widgets.BoundedIntText(value=2000, min=1, max=5000, step=1, description=\"Organisms\")\n",
    "    fam_w = widgets.BoundedIntText(value=10, min=1, max=200, step=1, description=\"Families\")\n",
    "    infile = widgets.Text(value=\"./data/full/uniprot-reviewed_yes.tab\",  description=\"Input path\")\n",
    "    outfile = widgets.Text(value=\"./data/data_file.fasta\", description=\"Output path\")\n",
    "\n",
    "    widget = interact_manual.options(manual_name=\"Prepare data\")\n",
    "    widget(data_preparation, n_org=org_w, n_fam=fam_w, file_path=infile, outfile_path=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_prepare_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At this point of code raw Swissprot data was filtered by the top number of organisms and picked proteins containing single domain from top biggest families.\n",
    "\n",
    "However, before splitting our data to train and test, we want to first adress very simmilar sequences.\n",
    "It can be done using CD-HIT package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_cdhit(input_f=\"./data/to_cluster/data_file.fasta\", output_f=\"./data/to_cluster/data_file_outttt.fasta\", c=\"0.99\"):\n",
    "    c = str(c)\n",
    "    print(\"Begin CD-HIT\")\n",
    "    os.system(f\"./CD-HIT/cd-hit -i {input_f} -o {output_f} -c {c}\")\n",
    "\n",
    "\n",
    "def run_cdhit_widget():\n",
    "    c = widgets.BoundedFloatText(value=0.99, min=0.7, max=1, description=\"Simmilarity\")\n",
    "    infile = widgets.Text(value=\"./data/data_file.fasta\",  description=\"Input path\")\n",
    "    outfile = widgets.Text(value=\"./data/clustering/data_file_clustered.fasta\", description=\"Output path\")\n",
    "\n",
    "    widget = interact_manual.options(manual_name=\"Prepare data\")\n",
    "    widget(run_cdhit, c=c, input_f=infile, output_f=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_cdhit_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now our data is cleaned out and ready to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data processing\n",
    "\n",
    "For this project data is processed in multiple ways.\n",
    "\n",
    " - Standard single-letter encoding\n",
    " - Conversion into numbers and using int-8 encoding\n",
    " - Conversion into 3-mers, encoding each 3-mer as a number and using int-16 encoding\n",
    " - Conversion into 3-mers, calculating count of the most popular triplets in sequences.\n",
    " - Conversion into k-mers, grouping with allowed edit distance and counting existance of fragments in group. (7-mers with edit distance 3)\n",
    " - Using Biovec vector encoder\n",
    "\n",
    "First we need to split our data into training and test parts.\n",
    "To avoid dominance of the biggest families, training data will contain the same number of sequences for each PFAM family.\n",
    "\n",
    "### Warning!\n",
    "Please analyse the histogram first before providing number of sequences per family to train.\n",
    "Families with number of representatives lower than provided will be filtered out!\n",
    "\n",
    "Sometimes it might be beneficial to filter out couple families with low coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def histogram_widget():\n",
    "    infile = widgets.Text(value=\"./data/clustering/data_file_clustered.fasta\",  description=\"Input path\")\n",
    "    widget = interact_manual.options(manual_name=\"Prepare histogram\")\n",
    "\n",
    "    widget(input_analysis, input_file=infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "histogram_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_to_train_test_widget():\n",
    "    train_val = widgets.BoundedIntText(value=790, min=1, max=5000, step=1, description=\"N. to train\")\n",
    "    infile = widgets.Text(value=\"./data/clustering/data_file_clustered.fasta\",  description=\"Input path\")\n",
    "    outfile = widgets.Text(value=\"./data/clean_dataset.pkl\", description=\"Output path\")\n",
    "\n",
    "    widget = interact_manual.options(manual_name=\"Prepare data\")\n",
    "    new_val = widget(split_to_train_test, train_val=train_val, infile=infile, outfile=outfile)\n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_val_global = split_to_train_test_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This process will save data into convenient pickle object. This way instead of keeping 4 separate files or trying to split our data in one file we can easily load a list prepared to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compress_protein_data_original(infile='./data/clean_dataset.pkl', outfile='./data/clean_dataset_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compress_protein_data_singletons(infile='./data/clean_dataset.pkl', outfile='./data/clean_dataset_singletons.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compress_protein_data_triplets(infile='./data/clean_dataset.pkl', outfile='./data/clean_dataset_triplets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compress_protein_data_sum_of_triplets(infile='./data/clean_dataset.pkl', outfile='./data/clean_dataset_sum_triplets.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Shorten sum k-mer\n",
    "\n",
    "Plugin below allows for customized summed k-mer encoding. Parameters:\n",
    "\n",
    "- input, output files\n",
    "- K-mer length - size of the moving, overalpping window\n",
    "- Min. occurences - How many times certain k-mer must exist in all sequences to be taken into consideration (filtering out highly mutated fragments, picking \"popularity\" strength)\n",
    "- Allowed edit distance - Fragments with edit distance equal or lower than provided to already found fragments will be united into one group. Example: with value 1 strings: AAAAA and AAAAB will be united while AAAAA and AAABB treated as separate groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def shorten_kmer_widget():\n",
    "    infile = widgets.Text(value=\"./data/clean_dataset.pkl\",  description=\"Input path\")\n",
    "    outfile = widgets.Text(value=\"./data/clean_dataset_sum_k_mers.pkl\", description=\"Output path\")\n",
    "    n_val = widgets.BoundedIntText(value=7, min=3, max=15, step=1, description=\"k-mer length\")\n",
    "    k_val = widgets.BoundedIntText(value=20, min=1, max=1000, step=1, description=\"min. occurences\")\n",
    "    edit_val = widgets.BoundedIntText(value=2, min=1, max=3, step=1, description=\"allowed edit distance\")\n",
    "\n",
    "    widget = interact_manual.options(manual_name=\"Prepare data\")\n",
    "    widget(compress_protein_data_sum_of_k_mers, n=n_val, k=k_val, edit=edit_val, infile=infile, outfile=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shorten_kmer_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Biovec\n",
    "\n",
    "Biovec model is built on top of the original implementation, thus data handling must be a bit different, to fit authors requirements.\n",
    "Sequences must be split into class fasta files, and one combined with identical names.\n",
    "Combined file will be used to generate model, wchich will be then saved.\n",
    "This saved model, is next loaded again, but this time, we also provide family information.\n",
    "\n",
    "After this procedure we are finally left with a data ready to split into training and test, perform learning and predictions.\n",
    "\n",
    "\n",
    "### Warning !\n",
    "This time widget is not provided, because of the ./data/vectors/combined_corpus.fasta file.\n",
    "If Biovec recognizes this file it will skip creating a new model.\n",
    "That's why paths here are fixed to ensure, old corpus file is deleted.\n",
    "\n",
    "### Warning !\n",
    "It might happen, that during loading process there will be information, that model did not train on certain triplets.\n",
    "It is connected with fragments containing extended alphabet like \"X\" and are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split_data_to_classes(infile=\"./data/clean_dataset.pkl\", output_folder=\"./data/vectors/class_folder\",\n",
    "                          output_combined_file=\"./data/vectors/combined.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prepare_biovec_model(infile=\"./data/vectors/combined.fasta\", outfile=\"./data/vectors/ProtVec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_biovec_widget():\n",
    "    train_val = widgets.BoundedIntText(value=790, min=1, max=5000, step=1, description=\"N. to train\")\n",
    "\n",
    "    widget = interact_manual.options(manual_name=\"Prepare data\")\n",
    "\n",
    "    new_val = widget(load_biovec_model_with_classes, train_size=train_val, input_model=fixed(\"./data/vectors/ProtVec_model.model\"), class_folder=fixed(\"./data/vectors/class_folder\"), outfile=fixed('./data/clean_dataset_biovec.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "load_biovec_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now, that we have all data prepared, we can evaluate them both in accuracy and runtime.\n",
    "\n",
    "Several models will be created - please note, that not all of them are equally suitable for this kind of data - the point is in efficiency comparison.\n",
    "\n",
    "- Decision trees\n",
    "- Random trees\n",
    "- Nearest Neighbours\n",
    "- MLP\n",
    "- Simple Machine Learning with Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_times = []\n",
    "all_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s1 = time()\n",
    "acc1, refit1 = decision_tree_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2, refit2 = decision_tree_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3, refit3 = decision_tree_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3\n",
    "\n",
    "s4 = time()\n",
    "acc4, refit4 = decision_tree_func(\"./data/clean_dataset_sum_triplets.pkl\")\n",
    "t4 = time() - s4\n",
    "\n",
    "s5 = time()\n",
    "acc5, refit5 = decision_tree_func(\"./data/clean_dataset_sum_k_mers.pkl\")\n",
    "t5 = time() - s5\n",
    "\n",
    "s6 = time()\n",
    "acc6, refit6 = decision_tree_func(\"./data/clean_dataset_biovec.pkl\")\n",
    "t6 = time() - s6\n",
    "\n",
    "t1 = round(t1, 2)\n",
    "t2 = round(t2, 2)\n",
    "t3 = round(t3, 2)\n",
    "t4 = round(t4, 2)\n",
    "t5 = round(t5, 2)\n",
    "t6 = round(t6, 2)\n",
    "\n",
    "all_times.append([t1, t2, t3, t4, t5, t6])\n",
    "all_accs.append([acc1, acc2, acc3, acc4, acc5, acc6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets | Sum Triplets | Sum K-mers | Biovec\")\n",
    "print(f\"----------|----------|------------|----------|--------------|------------|--------\")\n",
    "print(f\"Time      | {t1} \\t | {t2} \\t | {t3} \\t | {t4} \\t | {t5} \\t | {t6} \\t\")\n",
    "print(f\"Accuracy  | {acc1} \\t | {acc2} \\t | {acc3} \\t | {acc4} \\t | {acc5} \\t | {acc6} \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s1 = time()\n",
    "acc1, refit1 = random_tree_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2, refit2 = random_tree_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3, refit3 = random_tree_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3\n",
    "\n",
    "s4 = time()\n",
    "acc4, refit4 = random_tree_func(\"./data/clean_dataset_sum_triplets.pkl\")\n",
    "t4 = time() - s4\n",
    "\n",
    "s5 = time()\n",
    "acc5, refit5 = random_tree_func(\"./data/clean_dataset_sum_k_mers.pkl\")\n",
    "t5 = time() - s5\n",
    "\n",
    "s6 = time()\n",
    "acc6, refit6 = random_tree_func(\"./data/clean_dataset_biovec.pkl\")\n",
    "t6 = time() - s6\n",
    "\n",
    "t1 = round(t1, 2)\n",
    "t2 = round(t2, 2)\n",
    "t3 = round(t3, 2)\n",
    "t4 = round(t4, 2)\n",
    "t5 = round(t5, 2)\n",
    "t6 = round(t6, 2)\n",
    "\n",
    "all_times.append([t1, t2, t3, t4, t5, t6])\n",
    "all_accs.append([acc1, acc2, acc3, acc4, acc5, acc6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets | Sum Triplets | Sum K-mers | Biovec\")\n",
    "print(f\"----------|----------|------------|----------|--------------|------------|--------\")\n",
    "print(f\"Time      | {t1} \\t | {t2} \\t | {t3} \\t | {t4} \\t | {t5} \\t | {t6} \\t\")\n",
    "print(f\"Accuracy  | {acc1} \\t | {acc2} \\t | {acc3} \\t | {acc4} \\t | {acc5} \\t | {acc6} \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s1 = time()\n",
    "acc1, refit1 = nearest_neighbours_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2, refit2 = nearest_neighbours_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3, refit3 = nearest_neighbours_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3\n",
    "\n",
    "s4 = time()\n",
    "acc4, refit4 = nearest_neighbours_func(\"./data/clean_dataset_sum_triplets.pkl\")\n",
    "t4 = time() - s4\n",
    "\n",
    "s5 = time()\n",
    "acc5, refit5 = nearest_neighbours_func(\"./data/clean_dataset_sum_k_mers.pkl\")\n",
    "t5 = time() - s5\n",
    "\n",
    "s6 = time()\n",
    "acc6, refit6 = nearest_neighbours_func(\"./data/clean_dataset_biovec.pkl\")\n",
    "t6 = time() - s6\n",
    "\n",
    "t1 = round(t1, 2)\n",
    "t2 = round(t2, 2)\n",
    "t3 = round(t3, 2)\n",
    "t4 = round(t4, 2)\n",
    "t5 = round(t5, 2)\n",
    "t6 = round(t6, 2)\n",
    "\n",
    "all_times.append([t1, t2, t3, t4, t5, t6])\n",
    "all_accs.append([acc1, acc2, acc3, acc4, acc5, acc6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets | Sum Triplets | Sum K-mers | Biovec\")\n",
    "print(f\"----------|----------|------------|----------|--------------|------------|--------\")\n",
    "print(f\"Time      | {t1} \\t | {t2} \\t | {t3} \\t | {t4} \\t | {t5} \\t | {t6} \\t\")\n",
    "print(f\"Accuracy  | {acc1} \\t | {acc2} \\t | {acc3} \\t | {acc4} \\t | {acc5} \\t | {acc6} \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s1 = time()\n",
    "acc1, refit1 = mlp_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2, refit2 = mlp_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3, refit3 = mlp_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3\n",
    "\n",
    "s4 = time()\n",
    "acc4, refit4 = mlp_func(\"./data/clean_dataset_sum_triplets.pkl\")\n",
    "t4 = time() - s4\n",
    "\n",
    "s5 = time()\n",
    "acc5, refit5 = mlp_func(\"./data/clean_dataset_sum_k_mers.pkl\")\n",
    "t5 = time() - s5\n",
    "\n",
    "s6 = time()\n",
    "acc6, refit6 = mlp_func(\"./data/clean_dataset_biovec.pkl\")\n",
    "t6 = time() - s6\n",
    "\n",
    "t1 = round(t1, 2)\n",
    "t2 = round(t2, 2)\n",
    "t3 = round(t3, 2)\n",
    "t4 = round(t4, 2)\n",
    "t5 = round(t5, 2)\n",
    "t6 = round(t6, 2)\n",
    "\n",
    "all_times.append([t1, t2, t3, t4, t5, t6])\n",
    "all_accs.append([acc1, acc2, acc3, acc4, acc5, acc6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets | Sum Triplets | Sum K-mers | Biovec\")\n",
    "print(f\"----------|----------|------------|----------|--------------|------------|--------\")\n",
    "print(f\"Time      | {t1} \\t | {t2} \\t | {t3} \\t | {t4} \\t | {t5} \\t | {t6} \\t\")\n",
    "print(f\"Accuracy  | {acc1} \\t | {acc2} \\t | {acc3} \\t | {acc4} \\t | {acc5} \\t | {acc6} \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s1 = time()\n",
    "acc1, refit1 = deep_learning_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2, refit2 = deep_learning_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3, refit3 = deep_learning_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3\n",
    "\n",
    "s4 = time()\n",
    "acc4, refit4 = deep_learning_func(\"./data/clean_dataset_sum_triplets.pkl\")\n",
    "t4 = time() - s4\n",
    "\n",
    "s5 = time()\n",
    "acc5, refit5 = deep_learning_func(\"./data/clean_dataset_sum_k_mers.pkl\")\n",
    "t5 = time() - s5\n",
    "\n",
    "s6 = time()\n",
    "acc6, refit6 = deep_learning_func(\"./data/clean_dataset_biovec.pkl\")\n",
    "t6 = time() - s6\n",
    "\n",
    "t1 = round(t1, 2)\n",
    "t2 = round(t2, 2)\n",
    "t3 = round(t3, 2)\n",
    "t4 = round(t4, 2)\n",
    "t5 = round(t5, 2)\n",
    "t6 = round(t6, 2)\n",
    "\n",
    "all_times.append([t1, t2, t3, t4, t5, t6])\n",
    "all_accs.append([acc1, acc2, acc3, acc4, acc5, acc6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets | Sum Triplets | Sum K-mers | Biovec\")\n",
    "print(f\"----------|----------|------------|----------|--------------|------------|--------\")\n",
    "print(f\"Time      | {t1} \\t | {t2} \\t | {t3} \\t | {t4} \\t | {t5} \\t | {t6} \\t\")\n",
    "print(f\"Accuracy  | {acc1} \\t | {acc2} \\t | {acc3} \\t | {acc4} \\t | {acc5} \\t | {acc6} \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names = [\"Original\", \"Singletons\", \"Triplets\", \"Sum Triplets\", \"Sum K-mers\", \"Biovec\"]\n",
    "tests = [\"Decision trees\", \"Random trees\", \"Nearest neighbours\", \"MLP\", \"Machine Learning\"]\n",
    "print(all_times)\n",
    "print(all_accs)\n",
    "results_plot_benchmark(names, tests, all_times, all_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "files = [\"./data/clean_dataset_original.pkl\",\n",
    "         \"./data/clean_dataset_singletons.pkl\",\n",
    "         \"./data/clean_dataset_triplets.pkl\",\n",
    "         \"./data/clean_dataset_sum_triplets.pkl\",\n",
    "         \"./data/clean_dataset_sum_k_mers.pkl\",\n",
    "         \"./data/clean_dataset_biovec.pkl\"]\n",
    "\n",
    "plot_sizes(files, names, \"./presentation/images/sizes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}