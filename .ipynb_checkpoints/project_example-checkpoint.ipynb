{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep learning for Pfam family classification - benchmark comparison of sequence preprocessing effect on learning task\n",
    "\n",
    "Notebook below allows to play with parameters of the workflow and yield different results.\n",
    "As a default state it is set with parameters described in the paper as well as used as a base for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, interact_manual, fixed\n",
    "import sys\n",
    "from urllib import request\n",
    "from scripts.data_preparation_support import data_preparation, split_to_train_test\n",
    "from scripts.shorten_encoding import compress_protein_data_original, compress_protein_data_singletons, compress_protein_data_triplets\n",
    "from scripts.prepare_vectors import split_data_to_classes, prepare_biovec_model, load_biovec_model_with_classes\n",
    "from scripts.prepare_input_stats import input_analysis\n",
    "from scripts.model_scripts.decision_trees import decision_tree_func\n",
    "from scripts.model_scripts.random_tree import random_tree_func\n",
    "from scripts.model_scripts.mlp import mlp_func\n",
    "from scripts.model_scripts.nearest_neighbours import nearest_neighbours_func\n",
    "from scripts.model_scripts.deep_learning import deep_learning_func\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "In this project I provide already prepared - cleaned dataset used for the report, so download part below is commented out. However if desired, parameters can be tweaked to obtain altered version.\n",
    "\n",
    "Raw data for this project is easily accessible by the Swissprot part of the Uniprot Database.\n",
    "Unfortunately, full file weights around 250Mb, so instead I provide link for download.\n",
    "\n",
    "In case server can't handle the download through Python, it is still possiblle to download tab-separated file directly\n",
    "from the website: [LINK](https://www.uniprot.org/uniprot/?query=reviewed%3Ayes&columns=id%2Cdatabase(Pfam)%2Corganism%2Csequence)\n",
    "\n",
    "It suggested, although not neccessary, to put downloaded file into ./data/full directory for clarity and ease of use of the default values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# url=\"https://www.uniprot.org/uniprot/?query=reviewed:yes&format=tab&columns=id,database(Pfam),organism,sequence\"\n",
    "# request.urlretrieve(url, \"./data/full/uniprot-reviewed_yes.tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_prepare_widget():\n",
    "    org_w = widgets.BoundedIntText(value=3000, min=1, max=5000, step=1, description=\"Organisms\")\n",
    "    fam_w = widgets.BoundedIntText(value=50, min=1, max=200, step=1, description=\"Families\")\n",
    "    infile = widgets.Text(value=\"./data/full/uniprot-reviewed_yes.tab\",  description=\"Input path\")\n",
    "    outfile = widgets.Text(value=\"./data/data_file.fasta\", description=\"Output path\")\n",
    "\n",
    "    widget = interact_manual.options(manual_name=\"Prepare data\")\n",
    "    widget(data_preparation, n_org=org_w, n_fam=fam_w, file_path=infile, outfile_path=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb57b332bf94547afce86c3b450943f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='./data/full/uniprot-reviewed_yes.tab', description='Input path'), Text(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_prepare_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At this point of code raw Swissprot data was filtered by the top number of organisms and picked proteins containing single domain from top biggest families.\n",
    "\n",
    "However, before splitting our data to train and test, we want to first adress very simmilar sequences.\n",
    "It can be done using CD-HIT package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_cdhit(input_f=\"./data/to_cluster/data_file.fasta\", output_f=\"./data/to_cluster/data_file_outttt.fasta\", c=\"0.99\"):\n",
    "    c = str(c)\n",
    "    print(\"Begin CD-HIT\")\n",
    "    os.system(f\"./CD-HIT/cd-hit -i {input_f} -o {output_f} -c {c}\")\n",
    "\n",
    "\n",
    "def run_cdhit_widget():\n",
    "    c = widgets.BoundedFloatText(value=0.99, min=0.7, max=1, description=\"Simmilarity\")\n",
    "    infile = widgets.Text(value=\"./data/data_file.fasta\",  description=\"Input path\")\n",
    "    outfile = widgets.Text(value=\"./data/clustering/data_file_clustered.fasta\", description=\"Output path\")\n",
    "\n",
    "    widget = interact_manual.options(manual_name=\"Prepare data\")\n",
    "    widget(run_cdhit, c=c, input_f=infile, output_f=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a3620b5a0b4fafbe42f1d3e1ba041a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='./data/data_file.fasta', description='Input path'), Text(value='./data/clust…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_cdhit_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now our data is cleaned out and ready to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data processing\n",
    "\n",
    "For this project data is processed in multiple ways.\n",
    "\n",
    " - Standard single-letter encoding\n",
    " - Conversion into numbers and using int-8 encoding\n",
    " - Conversion into 3-mers, encoding each 3-mer as a number and using int-16 encoding\n",
    " - Using Biovec vector encoder\n",
    "\n",
    "First we need to split our data into training and test parts.\n",
    "To avoid dominance of the biggest families, training data will contain the same number of sequences for each PFAM family.\n",
    "\n",
    "### Warning!\n",
    "Please analyse the histogram first before providing number of sequences per family to train.\n",
    "Families with number of representatives lower than provided will be filtered out!\n",
    "\n",
    "Sometimes it might be beneficial to filter out couple families with low coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def histogram_widget():\n",
    "    infile = widgets.Text(value=\"./data/clustering/data_file_clustered.fasta\",  description=\"Input path\")\n",
    "    widget = interact_manual.options(manual_name=\"Prepare histogram\")\n",
    "\n",
    "    widget(input_analysis, input_file=infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc7f1801c1e4df0ad0b7e733683cf94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='./data/clustering/data_file_clustered.fasta', description='Input path'), But…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_to_train_test_widget():\n",
    "    train_val = widgets.BoundedIntText(value=50, min=1, max=5000, step=1, description=\"N. to train\")\n",
    "    infile = widgets.Text(value=\"./data/clustering/data_file_clustered.fasta\",  description=\"Input path\")\n",
    "    outfile = widgets.Text(value=\"./data/clean_dataset.pkl\", description=\"Output path\")\n",
    "\n",
    "    widget = interact_manual.options(manual_name=\"Prepare data\")\n",
    "    new_val = widget(split_to_train_test, train_val=train_val, infile=infile, outfile=outfile)\n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d88598775240efa2e6f87db7c8b038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(BoundedIntText(value=50, description='N. to train', max=5000, min=1), Text(value='./data…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_val_global = split_to_train_test_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This process will save data into convenient pickle object. This way instead of keeping 4 separate files or trying to split our data in one file we can easily load a list prepared to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compress_protein_data_original(infile='./data/clean_dataset.pkl', outfile='./data/clean_dataset_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compress_protein_data_singletons(infile='./data/clean_dataset.pkl', outfile='./data/clean_dataset_singletons.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compress_protein_data_triplets(infile='./data/clean_dataset.pkl', outfile='./data/clean_dataset_triplets.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Biovec\n",
    "\n",
    "Biovec model is built on top of the original implementation, thus data handling must be a bit different, to fit authors requirements.\n",
    "Sequences must be split into class fasta files, and one combined with identical names.\n",
    "Combined file will be used to generate model, wchich will be then saved.\n",
    "This saved model, is next loaded again, but this time, we also provide family information.\n",
    "\n",
    "After this procedure we are finally left with a data ready to split into training and test, perform learning and predictions.\n",
    "\n",
    "\n",
    "### Warning !\n",
    "This time widget is not provided, because of the ./data/vectors/combined_corpus.fasta file.\n",
    "If Biovec recognizes this file it will skip creating a new model.\n",
    "That's why paths here are fixed to ensure, old corpus file is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split_data_to_classes(infile=\"./data/clean_dataset.pkl\", output_folder=\"./data/vectors/class_folder\",\n",
    "                          output_combined_file=\"./data/vectors/combined.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prepare_biovec_model(infile=\"./data/vectors/combined.fasta\", outfile=\"./data/vectors/ProtVec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = load_biovec_model_with_classes(input_model=\"./data/vectors/ProtVec_model.model\", class_folder=\"./data/vectors/class_folder\", outfile='./data/clean_dataset_biovec.pkl', train_size=train_val_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now, that we have all data prepared, we can evaluate them both in accuracy and runtime.\n",
    "\n",
    "Several models will be created - please note, that not all of them are equally suitable for this kind of data - the point is in efficiency comparison.\n",
    "\n",
    "- Decision trees\n",
    "- Random trees\n",
    "- Nearest Neighbours\n",
    "- MLP\n",
    "- Simple Machine Learning with Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Best model:\n",
      "- max_depth: 100\n",
      "----- Model accuracy: 0.124\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Best model:\n",
      "- max_depth: 90\n",
      "----- Model accuracy: 0.651\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Best model:\n",
      "- max_depth: 100\n",
      "----- Model accuracy: 0.588\n"
     ]
    }
   ],
   "source": [
    "s1 = time()\n",
    "acc1 = decision_tree_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2 = decision_tree_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3 = decision_tree_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets\")\n",
    "print(f\"----------|----------|------------|----------\")\n",
    "print(f\"Time      | {t1}\\t | {t2}\\t | {t3} \\t\")\n",
    "print(f\"Accuracy  | {acc1}\\t | {acc2}\\t | {acc3} \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n",
      "Best model:\n",
      "- random_state: 1234\n",
      "- max_depth: 40\n",
      "- max_features: 15\n",
      "----- Model accuracy: 0.263\n",
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n",
      "Best model:\n",
      "- random_state: 1234\n",
      "- max_depth: 30\n",
      "- max_features: 20\n",
      "----- Model accuracy: 0.764\n",
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "s1 = time()\n",
    "acc1 = random_tree_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2 = random_tree_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3 = random_tree_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets\")\n",
    "print(f\"----------|----------|------------|----------\")\n",
    "print(f\"Time      | {t1}\\t | {t2}\\t | {t3} \\t\")\n",
    "print(f\"Accuracy  | {acc1}\\t | {acc2}\\t | {acc3} \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s1 = time()\n",
    "acc1 = nearest_neighbours_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2 = nearest_neighbours_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3 = nearest_neighbours_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets\")\n",
    "print(f\"----------|----------|------------|----------\")\n",
    "print(f\"Time      | {t1}\\t | {t2}\\t | {t3} \\t\")\n",
    "print(f\"Accuracy  | {acc1}\\t | {acc2}\\t | {acc3} \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s1 = time()\n",
    "acc1 = mlp_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2 = mlp_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3 = mlp_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets\")\n",
    "print(f\"----------|----------|------------|----------\")\n",
    "print(f\"Time      | {t1}\\t | {t2}\\t | {t3} \\t\")\n",
    "print(f\"Accuracy  | {acc1}\\t | {acc2}\\t | {acc3} \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s1 = time()\n",
    "acc1 = deep_learning_func(\"./data/clean_dataset_original.pkl\")\n",
    "t1 = time() - s1\n",
    "\n",
    "s2 = time()\n",
    "acc2 = deep_learning_func(\"./data/clean_dataset_singletons.pkl\")\n",
    "t2 = time() - s2\n",
    "\n",
    "s3 = time()\n",
    "acc3 = deep_learning_func(\"./data/clean_dataset_triplets.pkl\")\n",
    "t3 = time() - s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Statistic | Original | Singletons | Triplets\")\n",
    "print(f\"----------|----------|------------|----------\")\n",
    "print(f\"Time      | {t1}\\t | {t2}\\t | {t3} \\t\")\n",
    "print(f\"Accuracy  | {acc1}\\t | {acc2}\\t | {acc3} \\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
