\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\usepackage{times}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
 
\urlstyle{same}

\usepackage{datetime}
\newdateformat{specialdate}{\twodigit{\THEDAY}.\twodigit{\THEMONTH}.\THEYEAR}

\usepackage[utf8]{inputenc}

\usepackage{titling}

\usepackage{framed}
\usepackage[svgnames]{xcolor}
\colorlet{shadecolor}{Gainsboro!50}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{verbatim}

\usepackage{caption}
\captionsetup[figure]{name=Fig.}

\begin{document}
\begin{center}
\textbf{Benchmarking of sequence preprocessing step for family classification task}
\end{center}


\begin{center}
Sikora Maciej$^{1}$
\end{center}

[1] Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, Poland \newline

\section*{Abstract}
With the growing amount of protein data, there is a need for improving automated prediction models. However, more sophisticated ones are increasingly resource-heavy and slower, thus requiring preprocessing steps to yield satisfying results in finite time. Because successful compression of biological sequences should speed up the learning process without losing crucial information, standard string compression might not fit here. Instead, protein-dedicated models are required, taking advantage of the smaller amino acid alphabet as well as factoring wider biological context behind one-letter abbreviations.
Context analysing and learning models aren't a problem specific to protein data and are commonly used in natural language text analysis. Multiple such preprocessing models can be formulated as well as multiple algorithms can be used for the learning process depending on the research goal. Here I show a step-by-step comparison between several preprocessed protein datasets for the family classification tasks, evaluating with most common multi-class classifier algorithms with runtime and accuracy as metrics. Results show how important the sequence preparation step is, significantly improving model quality. We also highlight, that in pursuit of size reduction we shouldn't forget about accuracy assessment.

\section*{Introduction}
For the last 5 years unannotated part of the Uniprot database -- Swissprot, increased over 3 times from 73,711,881 (release 2017\_01) to 230,328,648 entries (2022\_01) [XXXXXX].

\section*{Methods and materials}
\section*{Results and discussion}
\section*{References}

\end{document}

\begin{comment}
BEGIN EXAMPLE PARAGRAPH:



-----Wstawianie obrazka-----
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{raw_url_fasta}
\caption{Przykład umiejscowienia sekwencji przy użyciu surowego linku.}
\end{center}
\end{figure}

-----Tabela-----
\begin{center}
\begin{tabular}{ | l | l | } 
\hline
Nazwa & Dodatek do linku\\ 
\hline
\hline
AC & id\\
\hline
Entry name & entry\%20name\\ 
\hline
Czy Reviewed & reviewed\\ 
\hline
Referencje do Pfam & database(Pfam)\\ 
\hline
Sekwencja & sequence\\ 
\hline
\end{tabular}
\end{center}

\end{comment}